{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_search(string, file):\n",
    "    \"\"\"Takes in string and file. Finds all lines where that string is. Returns a list of tuples.\n",
    "    the tuples have the line number and the line where the string appears.\n",
    "    \n",
    "    :param string: string to search for\n",
    "    :param file: location and filename to search\n",
    "    \n",
    "    :return: list of tuples, where the tuples have the line numner and line where the string appears\n",
    "    \"\"\"\n",
    "    \n",
    "    line_number = 0\n",
    "    list_of_results = []\n",
    "    with open(file, 'r') as read_obj:\n",
    "        for line in read_obj:\n",
    "            line_number += 1\n",
    "            if string in line:\n",
    "                list_of_results.append((line_number, line.rstrip()))\n",
    "                \n",
    "    return list_of_results\n",
    "\n",
    "def findData(filename, location):\n",
    "    \"\"\"Takes in a txt filename, along with the location on the user's computer. Will find\n",
    "    the parameters and the spike times. Returns a Data Frame with the spike times and a\n",
    "    list with the parameters.\n",
    "    \n",
    "    :param filename: file name of neuron\n",
    "    :param loaction: location of file\n",
    "    \n",
    "    :return: tuple with the dataframe with spike times and the list of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Finding kind and neuron name\n",
    "    neuron_name = \"\"\n",
    "    kind = \"\"\n",
    "    \n",
    "    # Kind\n",
    "    string = filename[::-1]\n",
    "    while (string[0] != '.'):\n",
    "        kind += string[0]\n",
    "        string = string[1:]\n",
    "    kind = kind[::-1]\n",
    "    \n",
    "    # Name\n",
    "    name = \"\"\n",
    "    string = string[1:]\n",
    "    while (string[0] != '/'):\n",
    "        name += string[0]\n",
    "        string = string[1:]\n",
    "    name = name[::-1]\n",
    "    name = name[:-2]\n",
    "    month = \"\"\n",
    "    day = \"\"\n",
    "    year = \"\"\n",
    "    string = filename[10:]\n",
    "    while (string[0] != '_'):\n",
    "        month += string[0]\n",
    "        string = string[1:]\n",
    "    string = string[1:]\n",
    "    while (string[0] != '_'):\n",
    "        day += string[0]\n",
    "        string = string[1:]\n",
    "    string = string[1:]\n",
    "    while (string[0] != '/'):\n",
    "        year += string[0]\n",
    "        string = string[1:]\n",
    "    date = month + '_' + day + '_' + year\n",
    "    neuron_name = name + \" \" + date\n",
    "    \n",
    "    file = location + filename\n",
    "    \n",
    "    # Getting all the lines with \"depvar= \" to be able to find the parameter list\n",
    "    list_of_results = string_to_search(\"depvar= \", file)\n",
    "    \n",
    "    paramList = []\n",
    "    for item in list_of_results:\n",
    "        string = item[1][8:]\n",
    "        val = \"\"\n",
    "        while string[0] != \" \":\n",
    "            val += string[0]\n",
    "            string = string[1:]\n",
    "        paramList.append(int(val))\n",
    "    \n",
    "    # Getting all the lines with \"nevents=\" to find the spike times\n",
    "    results = string_to_search(\"nevents=\", file)\n",
    "    list_of_dur = string_to_search(\"Dur=\", file)\n",
    "    dur = int(list_of_dur[0][1][4:])*10000\n",
    "    list_of_delay = string_to_search(\"Delay=\", file)\n",
    "    delay = int(list_of_delay[0][1][6:])*10000\n",
    "    \n",
    "    fileOpen = open(file)\n",
    "    all_lines = fileOpen.readlines()\n",
    "    fileOpen.close()\n",
    "    \n",
    "    spikeList = []\n",
    "    for item in results:\n",
    "        number = int(item[1][8:])\n",
    "        sub_list = []\n",
    "        for i in range(item[0], item[0] + number):\n",
    "            string = all_lines[i]\n",
    "            count = 0\n",
    "            first_char = string[count]\n",
    "            val = ''\n",
    "            while (first_char != '-'):\n",
    "                val += first_char\n",
    "                count += 1\n",
    "                first_char = string[count]\n",
    "            val = int(val)\n",
    "            if (val < (dur + delay)) and (val > delay):\n",
    "                sub_list.append(val/10000)\n",
    "        spikeList.append(sub_list)\n",
    "    d = {'spikes': spikeList}       \n",
    "    spikeDF = pd.DataFrame(d)\n",
    "\n",
    "    return spikeDF, paramList, neuron_name, kind\n",
    "\n",
    "def halfWidthFxn(y, x):\n",
    "    \"\"\" This function finds the halfwidth, and is used in returning the\n",
    "    infoList in the infoFunction. It takes in two lists, the mean count data\n",
    "    and the list of parameters. It returns a tuple with the HW, LL, UL, and the BV \n",
    "    (in that order). \"\"\"\n",
    "    \n",
    "    #Calculating Half Width\n",
    "    #Subtract off minimum\n",
    "\n",
    "    y = y - np.min(y)\n",
    "\n",
    "    #Rectify\n",
    "    y[y < 0] = 0\n",
    "    \n",
    "    #Normalize\n",
    "    y = y/np.max(y)\n",
    "\n",
    "    #Interpolate\n",
    "    xi = np.linspace(np.min(x),np.max(x),1001,endpoint=True)\n",
    "    yi = np.interp(xi, x, y)\n",
    "\n",
    "    #Find the maximum value of y\n",
    "    max_index = np.argmax(yi)\n",
    "\n",
    "    BV = xi[max_index]\n",
    "\n",
    "    tol = 0.01\n",
    "        \n",
    "    #Cut it in half\n",
    "\n",
    "    if max_index == 0:\n",
    "\n",
    "        LL = np.min(xi)\n",
    "\n",
    "        Ux = xi[max_index+1:]\n",
    "        Uy = yi[max_index+1:]\n",
    "\n",
    "        j = np.where(np.abs(Uy -.5) < tol)\n",
    "    \n",
    "        while (Ux[j].size <= 0):\n",
    "            tol += .01\n",
    "            j = np.where(np.abs(Uy -.5) < tol)\n",
    "        UL = np.min(Ux[j])\n",
    "        \n",
    "    elif max_index == np.size(xi)-1:\n",
    "\n",
    "        Lx = xi[1:max_index-1]\n",
    "        Ly = yi[1:max_index-1]\n",
    "\n",
    "        j = np.where(np.abs(Ly -.5) < tol)\n",
    "        \n",
    "        while (Lx[j].size <= 0):\n",
    "            tol += .01\n",
    "            j = np.where(np.abs(Ly -.5) < tol)\n",
    "        LL = np.min(Lx[j])\n",
    "\n",
    "        UL = np.size(xi)\n",
    "\n",
    "    else:\n",
    "\n",
    "        Lx = xi[1:max_index-1]\n",
    "        Ly = yi[1:max_index-1]\n",
    "\n",
    "        Ux = xi[max_index+1:]\n",
    "        Uy = yi[max_index+1:]\n",
    "\n",
    "        j = np.where(np.abs(Ly -.5) < tol)\n",
    "       \n",
    "        while (Lx[j].size <= 0):\n",
    "            tol += .01\n",
    "            j = np.where(np.abs(Ly -.5) < tol)\n",
    "            \n",
    "        LL = np.max(Lx[j])\n",
    "\n",
    "        j = np.where(np.abs(Uy -.5) < tol)\n",
    "        \n",
    "        tol = 0.01\n",
    "        while (Ux[j].size <= 0):\n",
    "            tol += .01\n",
    "            j = np.where(np.abs(Uy -.5) < tol)\n",
    "        \n",
    "        UL = np.min(Ux[j])\n",
    "    \n",
    "    #Get half width\n",
    "\n",
    "    HW=UL-LL\n",
    "    return HW, LL, UL, BV\n",
    "\n",
    "def infoFunction(data):\n",
    "    \"\"\" This function takes the data from txt file and returns a list\n",
    "    of the data values that are extracted. The peak, trough, best value,\n",
    "    and halfwidth. This function is used in the addingNeuron function. \"\"\"\n",
    "    \n",
    "    # Getting data from tuple\n",
    "    S = data[0]\n",
    "    parameters = np.array(data[1])\n",
    "    spikes = S.to_numpy()\n",
    "    \n",
    "    P = parameters.astype(np.float32)\n",
    "    \n",
    "    N_trial = np.size(spikes)\n",
    "\n",
    "    spike_count = np.zeros(N_trial)\n",
    "\n",
    "    for n in np.arange(N_trial):\n",
    "        spike_count[n] = np.size(spikes[n][0])\n",
    "\n",
    "    #Get unique parameter values\n",
    "    x = np.unique(parameters)\n",
    "\n",
    "    #Number of unique parameter values\n",
    "    N = np.size(x)\n",
    "    mean_count = np.zeros(N)\n",
    "    sd_count = np.zeros(N)\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        index = np.where(parameters == x[n])\n",
    "    \n",
    "        mean_count[n] = np.mean(spike_count[index])\n",
    "    \n",
    "        sd_count[n] = np.std(spike_count[index],ddof = 1)\n",
    "        \n",
    "    HW = round(halfWidthFxn(mean_count, x)[0], 2)\n",
    "    peak = round(np.max(mean_count), 2)\n",
    "    trough = round(np.min(mean_count), 2)\n",
    "    BV = round(halfWidthFxn(mean_count, x)[3], 2)\n",
    "    \n",
    "    infoList = [peak, trough, BV, HW, x, mean_count, sd_count]\n",
    "    \n",
    "    return infoList\n",
    "\n",
    "def tuningCurve(file, location):\n",
    "    \n",
    "    data = findData(file, location)\n",
    "    \n",
    "    # Getting data from tuple\n",
    "    S = data[0]\n",
    "    parameters = np.array(data[1])\n",
    "    spikes = S.to_numpy()\n",
    "    \n",
    "    P = parameters.astype(np.float32)\n",
    "    \n",
    "    N_trial = np.size(spikes)\n",
    "\n",
    "    spike_count = np.zeros(N_trial)\n",
    "\n",
    "    for n in np.arange(N_trial):\n",
    "        spike_count[n] = np.size(spikes[n][0])\n",
    "\n",
    "    #Get unique parameter values\n",
    "    x = np.unique(parameters)\n",
    "\n",
    "    #Number of unique parameter values\n",
    "    N = np.size(x)\n",
    "    mean_count = np.zeros(N)\n",
    "    sd_count = np.zeros(N)\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        index = np.where(parameters == x[n])\n",
    "    \n",
    "        mean_count[n] = np.mean(spike_count[index])\n",
    "    \n",
    "        sd_count[n] = np.std(spike_count[index],ddof = 1)\n",
    "     \n",
    "    #Calculating Half Width\n",
    "    HW = halfWidthFxn(mean_count, x)[0]\n",
    "    LL = halfWidthFxn(mean_count, x)[1]\n",
    "    UL = halfWidthFxn(mean_count, x)[2]\n",
    "    \n",
    "    neuron_name = data[2]\n",
    "    kind = data[3]\n",
    "        \n",
    "    units = \"\"\n",
    "    if kind == \"itd\":\n",
    "        units = ' ($\\mu$s)'\n",
    "    if kind == \"iid\":\n",
    "        units = ' (dB)'\n",
    "    if kind == \"bf\":\n",
    "        units = ' (kHz)'\n",
    "    \n",
    "    #Plot\n",
    "    plt.title(\"Neuron: \" + neuron_name, fontsize = 15) \n",
    "    plt.errorbar(x,mean_count,sd_count)\n",
    "    plt.xlabel(kind + units ,fontsize = 15)\n",
    "    plt.ylabel('Spike count',fontsize = 15)\n",
    "    plt.axvspan(LL, UL, facecolor='lightblue', alpha=0.5)\n",
    "    halfWidth = mpatches.Patch(color='lightblue', label='Half Width: ' + str(round(HW, 2)))\n",
    "    plt.legend(handles=[halfWidth])\n",
    "    plt.show()\n",
    "    \n",
    "def file_table():\n",
    "    \"\"\"Returns the filename dataframe. No parameters needed.\"\"\"\n",
    "        \n",
    "    table = pd.DataFrame(np.array([['670.01 12_4_06', 'LLDa/date/12_4_06/12_4_06_data/670.01.0.itd', 'LLDa/date/12_4_06/12_4_06_data/670.01.1.iid', 'LLDa/date/12_4_06/12_4_06_data/670.01.2.bf'],\n",
    "                          ['670.03 12_4_06', 'LLDa/date/12_4_06/12_4_06_data/670.03.0.itd', 'LLDa/date/12_4_06/12_4_06_data/670.03.1.iid', 'LLDa/date/12_4_06/12_4_06_data/670.03.2.bf'],\n",
    "                          ['670.04 12_4_06', 'LLDa/date/12_4_06/12_4_06_data/670.04.0.itd', 'LLDa/date/12_4_06/12_4_06_data/670.04.1.iid', 'LLDa/date/12_4_06/12_4_06_data/670.04.2.bf'],\n",
    "                          ['670.05 12_4_06', 'LLDa/date/12_4_06/12_4_06_data/670.05.0.itd', 'LLDa/date/12_4_06/12_4_06_data/670.05.1.iid', 'LLDa/date/12_4_06/12_4_06_data/670.05.2.bf'],\n",
    "                          ['670.06 12_4_06', 'LLDa/date/12_4_06/12_4_06_data/670.06.0.itd', None, 'LLDa/date/12_4_06/12_4_06_data/670.06.1.bf'],\n",
    "                          ['670.01 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.01.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.01.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.01.2.bf'],\n",
    "                          ['670.02 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.02.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.02.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.02.2.bf'],\n",
    "                          ['670.03 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.03.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.03.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.03.2.bf'],\n",
    "                          ['670.04 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.04.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.04.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.04.2.bf'],\n",
    "                          ['670.05 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.05.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.05.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.05.2.bf'],\n",
    "                          ['670.06 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.06.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.06.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.06.2.bf'],\n",
    "                          ['670.08 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.08.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.08.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.08.2.bf'],\n",
    "                          ['670.09 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.09.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.09.2.iid', 'LLDa/date/11_27_06/11_27_06_data/670.09.3.bf'],\n",
    "                          ['670.10 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.10.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.10.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.10.2.bf'],\n",
    "                          ['670.11 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.11.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.11.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.11.2.bf'],\n",
    "                          ['670.12 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.12.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.12.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.12.2.bf'],\n",
    "                          ['670.13 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.13.0.itd', None, 'LLDa/date/11_27_06/11_27_06_data/670.13.1.bf'],\n",
    "                          ['670.14 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.14.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.14.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.14.2.bf'],\n",
    "                          ['670.15 11_27_06', 'LLDa/date/11_27_06/11_27_06_data/670.15.0.itd', 'LLDa/date/11_27_06/11_27_06_data/670.15.1.iid', 'LLDa/date/11_27_06/11_27_06_data/670.15.2.bf'],\n",
    "                          ['664.01 10_5_06', 'LLDa/date/10_5_06/10_5_06_data/664.01.0.itd', 'LLDa/date/10_5_06/10_5_06_data/664.01.1.iid', 'LLDa/date/10_5_06/10_5_06_data/664.01.2.bf'],\n",
    "                          ['664.02 10_5_06', 'LLDa/date/10_5_06/10_5_06_data/664.02.1.itd', 'LLDa/date/10_5_06/10_5_06_data/664.02.0.iid', 'LLDa/date/10_5_06/10_5_06_data/664.02.2.bf'],\n",
    "                          ['664.04 10_5_06', None, None, 'LLDa/date/10_5_06/10_5_06_data/664.04.1.bf'],\n",
    "                          ['874.02 10_3_06', 'LLDa/date/10_3_06/10_3_06_data/874.02.0.itd', 'LLDa/date/10_3_06/10_3_06_data/874.02.1.iid', 'LLDa/date/10_3_06/10_3_06_data/874.02.2.bf'],\n",
    "                          ['874.05 10_3_06', 'LLDa/date/10_3_06/10_3_06_data/874.05.0.itd', 'LLDa/date/10_3_06/10_3_06_data/874.05.1.iid', 'LLDa/date/10_3_06/10_3_06_data/874.05.2.bf'],\n",
    "                          ['874.07 10_3_06', 'LLDa/date/10_3_06/10_3_06_data/874.07.0.itd', 'LLDa/date/10_3_06/10_3_06_data/874.07.1.iid', 'LLDa/date/10_3_06/10_3_06_data/874.07.2.bf'],\n",
    "                          ['874.06 9_25_06', 'LLDa/date/9_25_06/9_25_06_data/874.06.0.itd', 'LLDa/date/9_25_06/9_25_06_data/874.06.1.iid', 'LLDa/date/9_25_06/9_25_06_data/874.06.2.bf'],\n",
    "                          ['874.13 9_25_06', 'LLDa/date/9_25_06/9_25_06_data/874.13.0.itd', 'LLDa/date/9_25_06/9_25_06_data/874.13.1.iid', 'LLDa/date/9_25_06/9_25_06_data/874.13.2.bf'],\n",
    "                          ['874.03 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.03.0.itd', 'LLDa/date/9_6_06/9_6_06_data/874.03.1.iid', 'LLDa/date/9_6_06/9_6_06_data/874.03.2.bf'],\n",
    "                          ['874.06 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.06.0.itd', 'LLDa/date/9_6_06/9_6_06_data/874.06.1.iid', 'LLDa/date/9_6_06/9_6_06_data/874.06.2.bf'],\n",
    "                          ['874.07 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.07.0.itd', 'LLDa/date/9_6_06/9_6_06_data/874.07.1.iid', 'LLDa/date/9_6_06/9_6_06_data/874.07.2.bf'],\n",
    "                          ['874.09 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.09.2.itd', 'LLDa/date/9_6_06/9_6_06_data/874.09.3.iid', 'LLDa/date/9_6_06/9_6_06_data/874.09.5.bf'],\n",
    "                          ['874.10 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.10.0.itd', 'LLDa/date/9_6_06/9_6_06_data/874.10.1.iid', 'LLDa/date/9_6_06/9_6_06_data/874.10.2.bf'],\n",
    "                          ['874.12 9_6_06', 'LLDa/date/9_6_06/9_6_06_data/874.12.0.itd', 'LLDa/date/9_6_06/9_6_06_data/874.12.1.iid', 'LLDa/date/9_6_06/9_6_06_data/874.12.2.bf'],\n",
    "                          ['997.02 5_7_07', 'LLDa/date/5_7_07/5_7_07_data/997.02.0.itd', 'LLDa/date/5_7_07/5_7_07_data/997.02.1.iid', 'LLDa/date/5_7_07/5_7_07_data/997.02.2.bf'],\n",
    "                          ['997.02 3_26_07', 'LLDa/date/3_26_07/3_26_07_data/997.02.0.itd', 'LLDa/date/3_26_07/3_26_07_data/997.02.1.iid', 'LLDa/date/3_26_07/3_26_07_data/997.02.2.bf'],\n",
    "                          ['997.03 3_26_07', 'LLDa/date/3_26_07/3_26_07_data/997.03.0.itd', 'LLDa/date/3_26_07/3_26_07_data/997.03.1.iid', 'LLDa/date/3_26_07/3_26_07_data/997.03.2.bf'],\n",
    "                          ['997.04 3_26_07', 'LLDa/date/3_26_07/3_26_07_data/997.04.0.itd', 'LLDa/date/3_26_07/3_26_07_data/997.04.1.iid', 'LLDa/date/3_26_07/3_26_07_data/997.04.2.bf'],\n",
    "                          ['896.01 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.01.0.itd', 'LLDa/date/1_23_07/1_23_07_data/896.01.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.01.2.bf'],\n",
    "                          ['896.02 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.02.3.itd', 'LLDa/date/1_23_07/1_23_07_data/896.02.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.02.2.bf'],\n",
    "                          ['896.03 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.03.0.itd', 'LLDa/date/1_23_07/1_23_07_data/896.03.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.03.2.bf'],\n",
    "                          ['896.04 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.04.0.itd', 'LLDa/date/1_23_07/1_23_07_data/896.04.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.04.2.bf'],\n",
    "                          ['896.05 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.05.3.itd', 'LLDa/date/1_23_07/1_23_07_data/896.05.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.05.2.bf'],\n",
    "                          ['896.07 1_23_07', 'LLDa/date/1_23_07/1_23_07_data/896.07.2.itd', 'LLDa/date/1_23_07/1_23_07_data/896.07.1.iid', 'LLDa/date/1_23_07/1_23_07_data/896.07.4.bf'],\n",
    "                          ['995.01 1_8_07', 'LLDa/date/1_8_07/1_8_07_data/995.01.0.itd', 'LLDa/date/1_8_07/1_8_07_data/995.01.1.iid', 'LLDa/date/1_8_07/1_8_07_data/995.01.2.bf']]),\n",
    "                   columns=['name', 'itd', 'iid', 'bf'])\n",
    "    return table\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDataframe(location):\n",
    "    \"\"\"\"\"\"\n",
    "    files = file_table()\n",
    "    \n",
    "    names = []\n",
    "    itdPeak_list = []\n",
    "    itdTrough_list = []\n",
    "    itdBV_list = []\n",
    "    itdHW_list = []\n",
    "    iidPeak_list = []\n",
    "    iidTrough_list = []\n",
    "    iidBV_list = []\n",
    "    iidHW_list = []\n",
    "    bfPeak_list = []\n",
    "    bfTrough_list = []\n",
    "    bfBV_list = []\n",
    "    bfHW_list = []\n",
    "    \n",
    "    for index, row in files.iterrows():\n",
    "        # Setting names\n",
    "        names.append(row['name'])\n",
    "        \n",
    "        # Getting file names for each itd, iid, bf\n",
    "        itdFile = row['itd']\n",
    "        iidFile = row['iid']\n",
    "        bfFile = row['bf']\n",
    "        \n",
    "        # Adding itd data to lists\n",
    "        if itdFile == None:\n",
    "            itdPeak_list.append(None)\n",
    "            itdTrough_list.append(None)\n",
    "            itdBV_list.append(None)\n",
    "            itdHW_list.append(None)\n",
    "        else:\n",
    "            data = findData(itdFile, location)\n",
    "            info = infoFunction(data)\n",
    "        \n",
    "            itdPeak_list.append(info[0])\n",
    "            itdTrough_list.append(info[1])\n",
    "            itdBV_list.append(info[2])\n",
    "            itdHW_list.append(info[3])\n",
    "        \n",
    "        # Adding iid data to lists\n",
    "        if iidFile == None:\n",
    "            iidPeak_list.append(None)\n",
    "            iidTrough_list.append(None)\n",
    "            iidBV_list.append(None)\n",
    "            iidHW_list.append(None)\n",
    "        else:\n",
    "            data = findData(iidFile, location)\n",
    "            info = infoFunction(data)\n",
    "        \n",
    "            iidPeak_list.append(info[0])\n",
    "            iidTrough_list.append(info[1])\n",
    "            iidBV_list.append(info[2])\n",
    "            iidHW_list.append(info[3])\n",
    "        \n",
    "        # Adding bf data to lists\n",
    "        if bfFile == None:\n",
    "            bfPeak_list.append(None)\n",
    "            bfTrough_list.append(None)\n",
    "            bfBV_list.append(None)\n",
    "            bfHW_list.append(None)\n",
    "        else:\n",
    "            data = findData(bfFile, location)\n",
    "            info = infoFunction(data)\n",
    "        \n",
    "            bfPeak_list.append(info[0])\n",
    "            bfTrough_list.append(info[1])\n",
    "            bfBV_list.append(info[2])\n",
    "            bfHW_list.append(info[3])\n",
    "    \n",
    "    final = pd.DataFrame({\"Name\": names,\"itd Peak\": itdPeak_list, \"itd Trough\": itdTrough_list, \"itd Best Vaule\": itdBV_list, \"itd Halfwidth\": itdHW_list, \"iid Peak\": iidPeak_list, \"iid Trough\": iidTrough_list, \"iid Best Vaule\": iidBV_list, \"iid Halfwidth\": iidHW_list, \"bf Peak\": bfPeak_list, \"bf Trough\": bfTrough_list, \"bf Best Vaule\": bfBV_list, \"bf Halfwidth\": bfHW_list})\n",
    "    \n",
    "    return final\n",
    "\n",
    "def filename_search(name):\n",
    "    \"\"\"takes in the name of the nueron and searches through the dataframe to \n",
    "    return the filenmaes for itd, iid, and bf. Returns a list with these strings in that order. If\n",
    "    the file does not exist the list will hold the value None. If the name doesn't exist, will return\n",
    "    an empty list.\"\"\"\n",
    "    \n",
    "    table = file_table()\n",
    "    \n",
    "    if name in table.values:\n",
    "        index = table[table.name == name].index[0]\n",
    "        return [table.iloc[index, 1], table.iloc[index, 2], table.iloc[index, 3]]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def tuningCurveName(name, kind, location):\n",
    "     \n",
    "    alt_return = \"\"\n",
    "    \n",
    "    if kind == 'itd':\n",
    "        index = 0\n",
    "    elif kind == 'iid':\n",
    "        index = 1\n",
    "    elif kind == 'bf':\n",
    "        index = 2\n",
    "    else:\n",
    "        alt_return += \"Invalid kind\"\n",
    "    \n",
    "    file = filename_search(name)\n",
    "    if file == []:\n",
    "        if len(alt_return) > 0:\n",
    "            alt_return += \" and invalid name\"\n",
    "        else:\n",
    "            alt_return += \"Invalid name\"\n",
    "            \n",
    "    if len(alt_return) > 0:\n",
    "        return alt_return\n",
    "    else:\n",
    "        file = file[index]\n",
    "        return tuningCurve(file, location)\n",
    "    \n",
    "class Data:\n",
    "    def __init__(self, name, kind, location):\n",
    "        if kind == 'itd':\n",
    "            index = 0\n",
    "        elif kind == 'iid':\n",
    "            index = 1\n",
    "        elif kind == 'bf':\n",
    "            index = 2\n",
    "        file = filename_search(name)[index]\n",
    "        data = findData(file, location)\n",
    "        info = infoFunction(data)\n",
    "        self.peak = info[0]\n",
    "        self.trough = info[1]\n",
    "        self.BV = info[2]\n",
    "        self.HW = info[3]\n",
    "        self.parameters = info[4]\n",
    "        self.mean_count = info[5]\n",
    "        self.sd = info[6]\n",
    "\n",
    "def getData(name, kind, location):\n",
    "    return Data(name, kind, location)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
